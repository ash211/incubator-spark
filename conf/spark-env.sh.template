#!/usr/bin/env bash

# This file contains environment variables that affect Spark. Copy it as
# spark-env.sh and edit there to configure Spark for your site.
#
# The following variables can be set in this file:
# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
# - SPARK_JAVA_OPTS, to set node-specific JVM options for Spark. Note that
#     we recommend setting app-wide options in the application's driver program.
#       Examples of node-specific options : -Dspark.local.dir, GC options
#       Examples of app-wide options : -Dspark.serializer
#
# For use when deploying on Mesos:
# - MESOS_NATIVE_LIBRARY, to point to your libmesos.so
#
# For use with the standalone deploy mode:
# - SPARK_MASTER_IP, to bind the master to a different IP address or hostname
# - SPARK_MASTER_PORT, to change the port the worker listens on
# - SPARK_MASTER_WEBUI_PORT, to change the port the master web UI listens on
#
# - SPARK_WORKER_CORES, to set the number of cores to use on this machine
# - SPARK_WORKER_MEMORY, to set how much memory to use (e.g. 1000m, 2g)
# - SPARK_WORKER_PORT, to change the port the worker listens on
# - SPARK_WORKER_WEBUI_PORT, to change the port the worker web UI listens on
# - SPARK_WORKER_INSTANCES, to set the number of worker processes per node
#
# - SPARK_DAEMON_MEMORY, to configure heap size for the daemons themselves
# - SPARK_DAEMON_JAVA_OPTS, JVM options for the daemons themselves
#
# For use with Amazon AWS:
# - AWS_ACCESS_KEY_ID
# - AWS_SECRET_ACCESS_KEY
#
# For use with Hadoop:
# - HADOOP_CONF_DIR, to read the default filesystem name and other settings
#     from hdfs-site.xml and core-site.xml
#
# For use with PySpark:
# - PYSPARK_PYTHON, to point to the Python binary if python is not on your path
# - IPYTHON_OPTS, used to customize the ipython command
